{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install gTTS  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import pathlib\n",
    "import pickle\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from skimage import io \n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "from IPython import display\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers,Model,activations\n",
    "from keras.utils import plot_model\n",
    "from wordcloud import WordCloud\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3,preprocess_input\n",
    "from gtts import gTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Import the dataset and read the image into a seperate variable\n",
    "data = '/kaggle/input/flickr8k/'\n",
    "images= os.path.join(data,'Images/')\n",
    "\n",
    "all_imgs = glob.glob(images + '*.jpg',recursive=True)\n",
    "print(\"The total images present in the dataset: {}\".format(len(all_imgs)))\n",
    "all_imgs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_caption = pd.read_csv(\"/kaggle/input/flickr8k/captions.txt\")\n",
    "df_caption.head()\n",
    "len(df_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Visualise both the images & text present in the dataset\n",
    "def show_image_and_its_caption(index):\n",
    "    image_name, caption = df_caption.iloc[index][\"image\"], df_caption.iloc[index][\"caption\"]\n",
    "    img = io.imread(pathlib.Path(images,image_name))\n",
    "    plt.imshow(img)\n",
    "    print(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "show_image_and_its_caption(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Import the dataset and read the text file into a seperate variable\n",
    "text_file = 'captions.txt'\n",
    "def load_doc(filename):\n",
    "    \n",
    "    text = pd.read_csv(data + filename)\n",
    "    \n",
    "    return text\n",
    "\n",
    "doc = load_doc(text_file)\n",
    "print(doc[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "all_img_id = df_caption['image']\n",
    "all_img_vector = [data + 'Images/' + str(img) for img in all_img_id]\n",
    "annotations = df_caption['caption']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data = {'ID': all_img_id,'Path':all_img_vector,'Captions':annotations}\n",
    "df = pd.DataFrame(data=data)     \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Create a list which contains all the captions\n",
    "annotations= ['<start> '+ text +' <end>' for text in df.Captions]\n",
    "#add the <start> & <end> token to all those captions as well\n",
    "\n",
    "#Create a list which contains all the path to the images\n",
    "all_img_path= list(df.Path)\n",
    "\n",
    "print(\"Total captions present in the dataset: \"+ str(len(annotations)))\n",
    "print(\"Total images present in the dataset: \" + str(len(all_img_path)))\n",
    "print (annotations[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Create the vocabulary & the counter for the captions \n",
    "concat_str = ' '.join([str(text).strip('\\n') for text in annotations]) \n",
    "#print(concat_str[:500])\n",
    "vocabulary= concat_str.split(' ')\n",
    "print(vocabulary[:50])\n",
    "print(\"vocabulary length: \"+ str(len(set(vocabulary))))\n",
    "val_count=Counter(vocabulary)\n",
    "#val_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Visualise the top 30 occuring words in the captions\n",
    "wordcloud = WordCloud(\n",
    "                          background_color='white',\n",
    "                          max_words=30,\n",
    "                          max_font_size=40,\n",
    "                          random_state=42\n",
    "                         ).generate(str(annotations))\n",
    "\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Visualise the top 30 occuring words in the captions\n",
    "def plot_top_words(word_counter):\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    plt.figure(figsize=(14,4))\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.bar(*zip(*word_counter))\n",
    "    plt.title(\"Top 30 Occuring Words\")\n",
    "    plt.show()\n",
    "\n",
    "plot_top_words(val_count.most_common(30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# create the tokenizer\n",
    "top_word_cnt = 5000\n",
    "tokenizer = Tokenizer(num_words = top_word_cnt +1, filters= '!\"#$%^&*()_+.,:;-?/~`{}[]|\\=@ ',\n",
    "                      lower = True, char_level = False, \n",
    "                      oov_token = 'UNK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(annotations)\n",
    "#transform each text into a sequence of integers\n",
    "text_seqs = tokenizer.texts_to_sequences(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "text_seqs[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "index_word = tokenizer.index_word\n",
    "\n",
    "tokenizer.word_index['PAD'] = 0\n",
    "tokenizer.index_word[0] = 'PAD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(tokenizer.oov_token)\n",
    "print(tokenizer.index_word[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "word_count_dict = dict(tokenizer.word_counts.items())\n",
    "#word_count_dict\n",
    "len(word_count_dict)\n",
    "len(tokenizer.index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## Save Tokenizer\n",
    "with open('./tokenizer.pkl',\"wb\") as f:\n",
    "    pickle.dump(tokenizer,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create a word count of your tokenizer to visulize the Top 30 occuring words after text processing\n",
    "sort_word_by_count = sorted(tokenizer.word_counts.items(), key=lambda kv : kv[1], reverse= True)  #kv[1]: word frequency, reverse = True : desc order\n",
    "plot_top_words(sort_word_by_count[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color='black',max_words=30,\n",
    "                          max_font_size=40,\n",
    "                          random_state=42).generate_from_frequencies(word_count_dict)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Pad each vector to the max_length of the captions  store it to a vairable\n",
    "\n",
    "cap_vector= tf.keras.preprocessing.sequence.pad_sequences(text_seqs, padding= 'post', \n",
    "                                                          dtype='int32', value=0) \n",
    "print(\"The shape of Caption vector is :\" + str(cap_vector.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "IMAGE_SHAPE = (299,299)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#write your code here for creating the function. This function should return images & their path\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, IMAGE_SHAPE)\n",
    "    img = preprocess_input(img)\n",
    "    return img, image_path\n",
    "\n",
    "# Check the preprocessing Logic\n",
    "print(f'Size of image before preprocessing: {io.imread(all_img_vector[0]).shape}')\n",
    "print(f'Size of image after preprocessing: {preprocess_image(all_img_vector[0])[0].shape}')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "len(sorted(set(all_img_vector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#write your code here to create the dataset consisting of image paths\n",
    "#write your code here for applying the function to the image path dataset, such that the transformed dataset should contain images & their path\n",
    "\n",
    "unique_img_paths = sorted(set(all_img_vector))\n",
    "img_data = tf.data.Dataset.from_tensor_slices(unique_img_paths)\n",
    "print(img_data)\n",
    "img_data = img_data.map(preprocess_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(64)\n",
    "                \n",
    "img_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## Load the pretrained Imagenet weights of Inception net V3\n",
    "\n",
    "1.To save the memory(RAM) from getting exhausted, extract the features of the images using the last layer of pre-trained model. Including this as part of training will lead to higher computational time.\n",
    "\n",
    "2.The shape of the output of this layer is 8x8x2048. \n",
    "\n",
    "3.Use a function to extract the features of each image in the train & test dataset such that the shape of each image should be (batch_size, 8*8, 2048)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image_model = tf.keras.applications.InceptionV3(include_top=False,weights='imagenet')\n",
    "\n",
    "new_input = image_model.input  #write code here to get the input of the image_model\n",
    "hidden_layer = image_model.layers[-1].output  #write code here to get the output of the image_model\n",
    "\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer) #build the final model using both input & output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# write the code to apply the feature_extraction model to your earlier created dataset which contained images & their respective paths\n",
    "# Once the features are created, you need to reshape them such that feature shape is in order of (batch_size, 8*8, 2048)\n",
    "fv_dict = {}\n",
    "for img,path in tqdm(img_data):\n",
    "    fv_out = image_features_extract_model(img)\n",
    "    fv_flattened = tf.reshape(fv_out,(fv_out.shape[0],-1,fv_out.shape[3])) \n",
    "    for fv_img,path in zip(fv_flattened,path):\n",
    "        path = path.numpy().decode(\"utf-8\") #This decoding is done because the values are not in ASCII\n",
    "        fv_dict[path] = fv_img.numpy()  #This will convert the tensor instance to numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(fv_out.shape) # Last batch has only 27 images (126*64 + 27 = 8091)\n",
    "print(fv_out[0].shape)\n",
    "print(fv_flattened.shape)\n",
    "print(fv_flattened[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "len(fv_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#view top five items of features dict\n",
    "import more_itertools\n",
    "top_5 = more_itertools.take(5, fv_dict.items())\n",
    "top_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset creation\n",
    "1.Apply train_test_split on both image path & captions to create the train & test list. Create the train-test spliit using 80-20 ratio & random state = 42\n",
    "\n",
    "2.Create a function which maps the image path to their feature. \n",
    "\n",
    "3.Create a builder function to create train & test dataset & apply the function created earlier to transform the dataset\n",
    "\n",
    "2.Make sure you have done Shuffle and batch while building the dataset\n",
    "\n",
    "3.The shape of each image in the dataset after building should be (batch_size, 8*8, 2048)\n",
    "\n",
    "4.The shape of each caption in the dataset after building should be(batch_size, max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#write your code here\n",
    "\n",
    "path_train, path_test, cap_train, cap_test = train_test_split(all_img_vector, cap_vector, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Training data for images: \" + str(len(path_train)))\n",
    "print(\"Testing data for images: \" + str(len(path_test)))\n",
    "print(\"Training data for Captions: \" + str(len(cap_train)))\n",
    "print(\"Testing data for Captions: \" + str(len(cap_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create a function which maps the image path to their feature. \n",
    "# This function will take the image_path & caption and return it's feature & respective caption.\n",
    "\n",
    "def map_func(img_path, cap):\n",
    "    img_tensor = fv_dict[img_path.decode('utf-8')]\n",
    "    return img_tensor, cap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAQs on how to load the features:\n",
    "* You can load the features using the dictionary created earlier OR\n",
    "* You can store using numpy(np.load) to load the feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# create a builder function to create dataset which takes in the image path & captions as input\n",
    "# This function should transform the created dataset(img_path,cap) to (features,cap) using the map_func created earlier\n",
    "# tf.numpy_function() : Wraps a python function and uses it as a TensorFlow op.\n",
    "\n",
    "BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 64\n",
    "def gen_dataset(img_path, cap):\n",
    "        \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((img_path, cap))\n",
    "    dataset = dataset.map(lambda ele1, ele2 : tf.numpy_function(map_func, [ele1, ele2], [tf.float32, tf.int32]),num_parallel_calls = tf.data.experimental.AUTOTUNE)           \n",
    "    dataset = (dataset.shuffle(BUFFER_SIZE, reshuffle_each_iteration= True).batch(BATCH_SIZE , drop_remainder = False).prefetch(tf.data.experimental.AUTOTUNE))\n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset=gen_dataset(path_train,cap_train)\n",
    "test_dataset=gen_dataset(path_test,cap_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sample_img_batch, sample_cap_batch = next(iter(train_dataset))\n",
    "print(sample_img_batch.shape)  #(batch_size, 8*8, 2048)\n",
    "print(sample_cap_batch.shape) #(batch_size,max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "1.Set the parameters\n",
    "\n",
    "2.Build the Encoder, Attention model & Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "embedding_dim = 256 \n",
    "units = 512\n",
    "vocab_size = 5001 #top 5,000 words +1\n",
    "train_num_steps = len(path_train)  // BATCH_SIZE\n",
    "test_num_steps = len(path_test) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "test_num_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    def __init__(self,embed_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dense = layers.Dense(embed_dim) #build your Dense layer with relu activation\n",
    "        \n",
    "    def call(self, features):\n",
    "        features =  self.dense(features)# extract the features from the image shape: (batch, 8*8, embed_dim)\n",
    "        features = activations.relu(features)\n",
    "        return features\n",
    "    \n",
    "    def model(self):\n",
    "        features = keras.Input(shape = (64,2048), batch_size = 64)     \n",
    "        \n",
    "        return Model(inputs = features, outputs = self.call(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "encoder=Encoder(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plot_model(encoder.model(),to_file = 'encoder.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "encoder.model().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class Attention_model(Model):\n",
    "    def __init__(self, units):\n",
    "        super(Attention_model, self).__init__()\n",
    "        self.W1 =  layers.Dense(units)  #build your Dense layer for image feature vector\n",
    "        self.W2 =  layers.Dense(units)   #build your Dense layer for hidden state\n",
    "        self.V =   layers.Dense(1)  #build your final Dense layer with unit 1 for flattening after addition of fv and hidden state\n",
    "        self.units = units\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        #features shape: (batch_size, 8*8, embedding_dim). This is the output of Encoder.\n",
    "        # hidden shape: (batch_size, hidden_size)\n",
    "        hidden_with_time_axis =  tf.expand_dims(hidden, axis = 1)   # Expand the hidden shape to shape: (batch_size, 1, hidden_size)\n",
    "        score = self.V(activations.tanh(self.W1(features) + self.W2(hidden_with_time_axis)))  # build your score funciton to shape: (batch_size, 8*8, 1)\n",
    "        attention_weights = activations.softmax(score,axis = 1)   # extract your attention weights with shape: (batch_size, 8*8, 1)\n",
    "        context_vector = attention_weights * features   #shape: create the context vector with shape (batch_size, 8*8,embedding_dim)\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)  # reduce the shape to (batch_size, embedding_dim)\n",
    "        \n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "    def model(self):\n",
    "        features = keras.Input(shape = (64,256), batch_size = 64 ) \n",
    "        hidden = keras.Input(shape = (512), batch_size = 64)\n",
    "        \n",
    "        return Model(inputs = [features,hidden], outputs = self.call(features,hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "attention = Attention_model(units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "attention.model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plot_model(attention.model(), to_file = 'attention.png',show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "    def __init__(self, embed_dim, units, vocab_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.units = units\n",
    "        self.attention = Attention_model(self.units) #iniitalise your Attention model with units\n",
    "        self.embed =  layers.Embedding(input_dim = vocab_size, output_dim = embed_dim)  #build your Embedding layer\n",
    "        self.gru = tf.keras.layers.GRU(self.units,return_sequences=True,return_state=True,recurrent_initializer='glorot_uniform')\n",
    "        self.d1 = layers.Dense(self.units)  #build your Dense layer\n",
    "        self.d2 = layers.Dense(vocab_size)  #build your Dense layer\n",
    "        \n",
    "\n",
    "    def call(self,x,features, hidden):\n",
    "        context_vector, attention_weights = self.attention(features, hidden)  #create your context vector & attention weights from attention model\n",
    "        embed =  self.embed(x)  # embed your input to shape: (batch_size, 1, embedding_dim)\n",
    "        embed =  tf.concat([tf.expand_dims(context_vector, 1), embed], axis=-1) # Concatenate your input with the context vector from attention layer. Shape: (batch_size, 1, embedding_dim + embedding_dim)\n",
    "        output,state = self.gru(embed)  # Extract the output & hidden state from GRU layer. Output shape : (batch_size, max_length, hidden_size)\n",
    "        output = self.d1(output)\n",
    "        output = tf.reshape(output, (-1, output.shape[2])) # shape : (batch_size * max_length, hidden_size)\n",
    "        output = self.d2(output) # shape : (batch_size * max_length, vocab_size)\n",
    "        \n",
    "        return output,state, attention_weights\n",
    "    \n",
    "    def init_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))  # Initial hidden state (shape = 512)\n",
    "    \n",
    "    def model(self):\n",
    "        x = keras.Input(shape = (1), batch_size = 64)  # shape=1 refers to the word_index of the word at that timestamp\n",
    "        features = keras.Input(shape = (64,256), batch_size = 64 ) \n",
    "        hidden = keras.Input(shape = (512), batch_size = 64)\n",
    "        \n",
    "        return Model(inputs = [x,features,hidden], outputs = self.call(x,features,hidden))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "decoder=Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "decoder.model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plot_model(decoder.model(),to_file = 'decoder.png' ,show_shapes = True,show_layer_activations=True,\n",
    "    show_trainable=True, expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#This section verifies the models build, using a sample batch\n",
    "\n",
    "features=encoder(sample_img_batch)\n",
    "hidden = decoder.init_state(batch_size=sample_cap_batch.shape[0])\n",
    "dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * sample_cap_batch.shape[0], 1)  # Providing 64 <start> tokens to indicate start of 64 captions\n",
    "\n",
    "predictions, hidden_out, attention_weights= decoder(dec_input, features, hidden)\n",
    "\n",
    "print('Feature shape from Encoder: {}'.format(features.shape)) #(batch, 8*8, embed_dim)\n",
    "print('Shape of hidden: {}'.format(hidden.shape)) \n",
    "print('Shape of dec_input: {}'.format(dec_input.shape))\n",
    "print('Predictions shape from Decoder: {}'.format(predictions.shape)) #(batch,vocab_size)\n",
    "print('Shape of hidden_out from Decoder: {}'.format(hidden_out.shape))\n",
    "print('Attention weights shape from Decoder: {}'.format(attention_weights.shape)) #(batch, 8*8, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## Model training & optimization\n",
    "1.Set the optimizer & loss object\n",
    "\n",
    "2.Create your checkpoint path\n",
    "\n",
    "3.Create your training & testing step functions\n",
    "\n",
    "4.Create your loss function for the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.001)  \n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')  #Since decoder does not have a softmax therefore, logits = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!ls /kaggle/working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"/kaggle/working/checkpoints/\"):\n",
    "    os.mkdir(\"/kaggle/working/checkpoints/\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"/kaggle/working/checkpoints/\"\n",
    "#checkpoint_path = os.path.join(\"checkpoints\",\"train\")\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[link text](https://)* While creating the training step for your model, you will apply Teacher forcing.\n",
    "* Teacher forcing is a technique where the target/real word is passed as the next input to the decoder instead of previous prediciton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):   \n",
    "    loss = 0\n",
    "    hidden = decoder.init_state(batch_size=target.shape[0])\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(img_tensor)\n",
    "        for i in range(1,target.shape[1]):\n",
    "            predictions,hidden,_ = decoder(dec_input, features, hidden)\n",
    "            loss += loss_function(target[:, i], predictions)  \n",
    "            dec_input = tf.expand_dims(target[:, i], 1)  #Teacher enforcing\n",
    "        avg_loss = (loss/target.shape[1])  #Avg loss over a caption\n",
    "        trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "           \n",
    "    return loss, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(img_tensor, target):\n",
    "    loss = 0\n",
    "    hidden = decoder.init_state(batch_size=target.shape[0])\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "    features = encoder(img_tensor)\n",
    "    for i in range(1,target.shape[1]):\n",
    "        predictions,hidden,_ = decoder(dec_input, features, hidden)\n",
    "        loss += loss_function(target[:, i], predictions) \n",
    "        predicted_id = tf.argmax(predictions,1)  # No Teacher enforcing\n",
    "        dec_input = tf.expand_dims(predicted_id, 1)\n",
    "         \n",
    "    avg_loss = (loss/target.shape[1])  #Avg loss over a batch   \n",
    "        \n",
    "    return loss, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def test_loss_cal(test_dataset):\n",
    "    total_loss = 0\n",
    "    for (batch, (img_tensor, target)) in enumerate(test_dataset):\n",
    "        batch_loss, t_loss = test_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "    avg_test_loss=total_loss/test_num_steps\n",
    "    \n",
    "    return avg_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "loss_plot = []\n",
    "test_loss_plot = []\n",
    "EPOCHS = 15\n",
    "\n",
    "best_test_loss=100\n",
    "for epoch in tqdm(range(0, EPOCHS)):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(train_dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "        avg_train_loss=total_loss / train_num_steps\n",
    "        \n",
    "    loss_plot.append(avg_train_loss)    \n",
    "    test_loss = test_loss_cal(test_dataset)\n",
    "    test_loss_plot.append(test_loss)\n",
    "    \n",
    "    print ('For epoch: {}, the train loss is {:.3f}, & test loss is {:.3f}'.format(epoch+1,avg_train_loss,test_loss))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "    \n",
    "    if test_loss < best_test_loss:\n",
    "        print('Test loss has been reduced from %.3f to %.3f' % (best_test_loss, test_loss))\n",
    "        best_test_loss = test_loss\n",
    "                \n",
    "        ckpt_manager.save()\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.plot(test_loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!ls\n",
    "!cd ./checkpoints/\n",
    "!ls checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!cat ./checkpoints/checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print('shpe of loss_plot', loss_plot)\n",
    "print('shpe of test_loss_plot', test_loss_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "1.Define your evaluation function using greedy search\n",
    "\n",
    "2.Define your evaluation function using beam search ( optional)\n",
    "\n",
    "3.Test it on a sample data using BLEU score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "attention_features_shape = 64\n",
    "max_seq_length = cap_vector.shape[1]\n",
    "def evaluate(image):\n",
    "    attention_plot = np.zeros((max_seq_length, attention_features_shape))\n",
    "\n",
    "    hidden = decoder.init_state(batch_size=1)\n",
    "\n",
    "    temp_input = tf.expand_dims(preprocess_image(image)[0], 0) #process the input image to desired format before extracting features\n",
    "    img_tensor_val = image_features_extract_model(temp_input)  # Extract features using our feature extraction model\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "    features = encoder(img_tensor_val)  # extract the features by passing the input to encoder\n",
    "    #print(\"shape of features\", features.shape)\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    #print(\"shape of first dec_input\", dec_input.shape)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_seq_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input,\n",
    "                                                         features,\n",
    "                                                         hidden)# get the output from decoder\n",
    "        #print(\"shape of predictions\", predictions.shape)\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()  \n",
    "        predicted_id = (tf.argmax(predictions,1)).numpy()[0]  #extract the predicted id(index value from tokenizer) which carries the max value\n",
    "        #print(\"predicted_id:\", predicted_id)\n",
    "        #map the id to the word from tokenizer and append the value to the result list\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "        \n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, attention_plot,predictions\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "        \n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot,predictions\n",
    "\n",
    "'''\n",
    "Shape of features: (1,64,256)\n",
    "Shape of first dec_input: (1,1)\n",
    "Shape of predictions: (1,5001)\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "attention_features_shape = 64\n",
    "max_seq_length = cap_vector.shape[1]\n",
    "def beam_evaluate(image, beam_index = 3):\n",
    "\n",
    "    start = [tokenizer.word_index['<start>']]\n",
    "    result = [[start, 0.0]]\n",
    "\n",
    "    attention_plot = np.zeros((max_seq_length, attention_features_shape))\n",
    "\n",
    "    hidden = decoder.init_state(batch_size=1)\n",
    "\n",
    "    temp_input = tf.expand_dims(preprocess_image(image)[0], 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "    features = encoder(img_tensor_val)\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    while len(result[0][0]) < max_seq_length:\n",
    "        temp = []\n",
    "        for i, s in enumerate(result):\n",
    "            predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "            attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "            word_preds = np.argsort(predictions[0])[-beam_index:]\n",
    "\n",
    "            for w in word_preds:\n",
    "                next_cap, prob = s[0][:], s[1]\n",
    "                next_cap.append(w)\n",
    "\n",
    "                prob += np.log(predictions[0][w])\n",
    "\n",
    "                temp.append([next_cap, prob])\n",
    "        result = temp\n",
    "        result = sorted(result, reverse=False, key=lambda l: l[1])\n",
    "        result = result[-beam_index:]\n",
    "\n",
    "\n",
    "        predicted_id = result[-1]\n",
    "        pred_list = predicted_id[0]\n",
    "\n",
    "        prd_id = pred_list[-1]\n",
    "        if(prd_id!=3):\n",
    "            dec_input = tf.expand_dims([prd_id], 0)  \n",
    "        else:\n",
    "            break\n",
    "\n",
    "\n",
    "    result2 = result[-1][0]\n",
    "\n",
    "    intermediate_caption = [tokenizer.index_word[i] for i in result2]\n",
    "    final_caption = []\n",
    "    for i in intermediate_caption:\n",
    "        if i != '<end>':\n",
    "            final_caption.append(i)\n",
    "\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    final_caption = ' '.join(final_caption[1:])\n",
    "    return final_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "test_image = \"/kaggle/input/flickr8k/Images/3106883334_419f3fb16f.jpg\"\n",
    "captions=beam_evaluate(test_image)\n",
    "print(captions)\n",
    "Image.open(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def plot_attmap(caption, weights, image):\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    temp_img = np.array(Image.open(image))\n",
    "    \n",
    "    len_cap = len(caption)\n",
    "    for cap in range(len_cap):\n",
    "        weights_img = np.reshape(weights[cap], (8,8))\n",
    "        weights_img = np.array(Image.fromarray(weights_img).resize((224, 224), Image.LANCZOS))\n",
    "        \n",
    "        ax = fig.add_subplot(len_cap//2, len_cap//2, cap+1)\n",
    "        ax.set_title(caption[cap], fontsize=15)\n",
    "        \n",
    "        img=ax.imshow(temp_img)\n",
    "        \n",
    "        ax.imshow(weights_img, cmap='gist_heat', alpha=0.6,extent=img.get_extent())\n",
    "        ax.axis('off')\n",
    "    plt.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def filt_text(text):\n",
    "    filt=['<start>','UNK','<end>'] \n",
    "    temp= text.split()\n",
    "    [temp.remove(j) for k in filt for j in temp if k==j]\n",
    "    text=' '.join(temp)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def test_caption_generation(img_test_batch):\n",
    "    rid = np.random.randint(0, len(img_test_batch))\n",
    "    test_image = img_test_batch[rid]\n",
    "#test_image = './images/413231421_43833a11f5.jpg'\n",
    "#real_caption = '<start> black dog is digging in the snow <end>'\n",
    "\n",
    "    real_caption = ' '.join([tokenizer.index_word[i] for i in cap_test[rid] if i not in [0]])\n",
    "    result, attention_plot,pred_test = evaluate(test_image)\n",
    "\n",
    "\n",
    "    real_caption=filt_text(real_caption)      \n",
    "\n",
    "\n",
    "    pred_caption=' '.join(result).rsplit(' ', 1)[0]\n",
    "      \n",
    "    real_appn = []\n",
    "    real_appn.append(real_caption.split())\n",
    "    reference = real_appn\n",
    "    candidate = pred_caption.split()\n",
    "\n",
    "    score = sentence_bleu(reference, candidate, weights= (0.5,0.5,0,0))  #set your weights\n",
    "    print(f\"BELU score: {score*100}\")\n",
    "\n",
    "    print ('Real Caption:', real_caption)\n",
    "    print ('Prediction Caption:', pred_caption)\n",
    "    plot_attmap(result, attention_plot, test_image)\n",
    "    \n",
    "    Image.open(test_image)\n",
    "    \n",
    "    ##Converting text to speech\n",
    "    tts = gTTS(pred_caption, lang = 'en', slow = False)\n",
    "    #This can be downloaded and played\n",
    "    filename = '/kaggle/working/caption.mp3'\n",
    "    tts.save(filename) \n",
    "    display.display(display.Audio(filename, rate = None, autoplay = False)) # To display playback bar as output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "test_caption_generation(path_test)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
